name: AI Verification & Site Update

on:
  schedule:
    - cron: '0 */6 * * *' # Esegue ogni 6 ore
  workflow_dispatch:      # Esegue quando premi il bottone manuale

permissions:
  contents: write

jobs:
  ai-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install pandas requests numpy gspread google-auth tavily-python openai

      # FASE 1: L'Agente AI controlla e aggiorna Google Sheets (Opzionale, se lo usi)
      - name: ü§ñ Run AI Agent
        env:
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
        run: python3 scripts/ai_agent.py || echo "AI Agent finished with warnings (continuing...)"

      # FASE 2: Scarica i dati e genera il sito
      - name: üîÑ Download & Generate Data
        run: |
          python3 <<'EOF'
          import pandas as pd
          import json
          import sys
          import numpy as np
          import requests
          import io

          # --- CONFIGURAZIONE ---
          # Assicurati che il link sia corretto e pubblico (o gestito via token)
          SHEET_URL = "https://docs.google.com/spreadsheets/d/1NEyNXzCSprGOw6gCmVVbtwvFmz8160Oag-WqG93ouoQ/export?format=csv"
          
          # MAPPATURA COLONNE (Da Intestazione Sheet -> A Nome Variabile Python)
          COLUMN_MAPPING = {
              "latitude": "latitude", "lat": "latitude",
              "longitude": "longitude", "lon": "longitude", "long": "longitude",
              "title": "title", "titolo": "title",
              "date": "date", "data": "date",
              "type": "type", "type of attack": "type", "tipo": "type",
              "location": "location", "luogo": "location",
              "source": "link", "fonte": "link",
              "archived": "archived", "archivio": "archived",
              "verification": "verification", "verifica": "verification",
              "notes": "description", "note": "description",
              "description": "description", "descrizione": "description",
              "video": "video", "video_url": "video",
              "intensity": "intensity", "intensit√†": "intensity",
              # --- NUOVA COLONNA CRITICA PER I FILTRI ---
              "actor_code": "actor_code", "codice attore": "actor_code", "actor code": "actor_code"
          }

          print("üîÑ Scaricamento CSV da Google Sheets...")
          try:
              response = requests.get(SHEET_URL)
              response.raise_for_status()
              df = pd.read_csv(io.StringIO(response.text))
          except Exception as e:
              print(f"‚ùå Errore download: {e}")
              sys.exit(1)

          # Normalizzazione Intestazioni (tutto minuscolo e senza spazi extra)
          df.columns = df.columns.str.strip().str.lower()
          df.rename(columns=COLUMN_MAPPING, inplace=True)
          
          # Rimuove colonne duplicate se presenti
          df = df.loc[:, ~df.columns.duplicated()]
          
          # Sostituisce NaN con None per compatibilit√† JSON
          df = df.replace({np.nan: None})

          # --- 1. GENERAZIONE EVENTS.GEOJSON (Mappa) ---
          features = []
          for _, row in df.iterrows():
              lat = row.get("latitude")
              lon = row.get("longitude")
              
              if lat is None or lon is None: continue

              ver = row.get("verification", "not verified")
              
              # Calcolo Intensity (Fallback se vuoto)
              try:
                  raw_intensity = row.get("intensity")
                  if raw_intensity is not None and raw_intensity != "":
                      intensity = float(raw_intensity)
                  else:
                      intensity = 0.7 if ver == "verified" else 0.2
              except:
                  intensity = 0.2

              # Gestione Actor Code (Fondamentale per i filtri)
              actor_code = row.get("actor_code")
              if actor_code:
                  actor_code = str(actor_code).upper().strip()
              else:
                  actor_code = "UNK"

              props = {
                  "title": row.get("title") or "Evento",
                  "date": str(row.get("date") or ""),
                  "type": row.get("type") or "Drones",
                  "location": row.get("location") or "",
                  "link": row.get("link") or "",
                  "archived": row.get("archived") or "",
                  "verification": ver,
                  "description": row.get("description") or "",
                  "video": row.get("video") or "",
                  "intensity": intensity,
                  "actor_code": actor_code  # <--- INSERITO NEL GEOJSON
              }
              
              features.append({
                  "type": "Feature",
                  "geometry": {"type": "Point", "coordinates": [float(lon), float(lat)]},
                  "properties": props
              })

          with open("assets/data/events.geojson", "w", encoding="utf-8") as f:
              json.dump({"type": "FeatureCollection", "features": features}, f, ensure_ascii=False, indent=2)
          print(f"‚úÖ events.geojson generato con {len(features)} eventi.")

          # --- 2. GENERAZIONE EVENTS_TIMELINE.JSON (TimelineJS) ---
          tl_events = []
          for _, row in df.iterrows():
              try:
                  d_str = str(row.get("date", ""))
                  # Parsing Data (YYYY-MM-DD o DD/MM/YYYY)
                  year, month, day = "", "", ""
                  if '/' in d_str:
                      parts = d_str.split('/')
                      if len(parts) == 3: day, month, year = map(str.strip, parts)
                      if len(year) == 2: year = "20" + year
                  elif '-' in d_str:
                      parts = d_str.split('-')
                      if len(parts) == 3: year, month, day = map(str.strip, parts)
                  else: continue # Salta se data invalida

                  # Testo HTML per la slide
                  desc = row.get("description") or ""
                  # Aggiungiamo il codice attore anche qui per riferimento
                  act_code = str(row.get("actor_code") or "UNK").upper()
                  
                  html = f"<b>Tipo:</b> {row.get('type')}<br><b>Attore:</b> {act_code}<br><b>Luogo:</b> {row.get('location')}"
                  if desc:
                      html += f"<br><br>{desc}"
                  
                  # Oggetto Evento Timeline
                  event_obj = {
                      "start_date": {"year": int(year), "month": int(month), "day": int(day)},
                      "text": {
                          "headline": row.get("title") or "Evento",
                          "text": html
                      },
                      "group": row.get("type") or "Generale",
                      "unique_id": str(row.name if hasattr(row, 'name') else ''), 
                  }

                  # Media (Video/Foto)
                  video_url = row.get("video")
                  if video_url:
                      event_obj["media"] = {
                          "url": video_url,
                          "caption": row.get("source") or "Fonte",
                          "credit": "OSINT Source"
                      }
                  
                  tl_events.append(event_obj)
              except: continue

          final_json = {
              "title": {
                  "text": {
                      "headline": "Impact Atlas Timeline",
                      "text": "Cronologia interattiva degli eventi verificati."
                  }
              },
              "events": tl_events
          }

          with open("assets/data/events_timeline.json", "w", encoding="utf-8") as f:
              json.dump(final_json, f, ensure_ascii=False, indent=2)
          
          print(f"‚úÖ events_timeline.json generato con {len(tl_events)} eventi.")
          EOF

      - name: Commit and push updates
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          
          git add assets/data/events.geojson assets/data/events_timeline.json
          
          if git diff --quiet && git diff --staged --quiet; then
            echo "Nessuna modifica rilevata, salto il commit."
            exit 0
          fi
          
          git commit -m "Auto-update: AI + Data Refresh + Actor Codes"
          
          git pull --rebase origin main
          git push origin main
