name: AI Verification & Site Update

on:
  schedule:
    - cron: '0 */6 * * *' # Esegue ogni 6 ore
  workflow_dispatch:      # Esegue quando premi il bottone manuale

permissions:
  contents: write

jobs:
  ai-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install pandas requests numpy gspread google-auth tavily-python openai

      # FASE 1: L'Agente AI controlla e aggiorna Google Sheets
      - name: ðŸ¤– Run AI Agent
        env:
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
        run: python3 scripts/ai_agent.py || echo "AI Agent finished with warnings (continuing...)"

      # FASE 2: Scarica i dati (anche quelli appena aggiornati dall'AI) e genera il sito
      - name: ðŸ”„ Download & Generate Data
        run: |
          python3 <<'EOF'
          import pandas as pd
          import json
          import sys
          import numpy as np
          import requests
          import io

          # --- CONFIGURAZIONE ---
          SHEET_URL = "https://docs.google.com/spreadsheets/d/1NEyNXzCSprGOw6gCmVVbtwvFmz8160Oag-WqG93ouoQ/export?format=csv"
          
          COLUMN_MAPPING = {
              "latitude": "latitude", "lat": "latitude",
              "longitude": "longitude", "lon": "longitude", "long": "longitude",
              "title": "title", "titolo": "title",
              "date": "date", "data": "date",
              "type": "type", "type of attack": "type", "tipo": "type",
              "location": "location", "luogo": "location",
              "source": "link", "fonte": "link",
              "archived": "archived", "archivio": "archived",
              "verification": "verification", "verifica": "verification",
              "notes": "description", "note": "description"
          }

          print("ðŸ”„ Scaricamento CSV da Google Sheets...")
          try:
              response = requests.get(SHEET_URL)
              response.raise_for_status()
              df = pd.read_csv(io.StringIO(response.text))
          except Exception as e:
              print(f"âŒ Errore download: {e}")
              sys.exit(1)

          # Normalizzazione
          df.columns = df.columns.str.strip().str.lower()
          df.rename(columns=COLUMN_MAPPING, inplace=True)
          df = df.replace({np.nan: None})

          # --- EVENTS.GEOJSON ---
          features = []
          for _, row in df.iterrows():
              lat = row.get("latitude")
              lon = row.get("longitude")
              
              if lat is None or lon is None: continue

              ver = row.get("verification", "not verified")
              intensity = 0.7 if ver == "verified" else 0.2

              props = {
                  "title": row.get("title") or "Evento",
                  "date": str(row.get("date") or ""),
                  "type": row.get("type") or "Drones",
                  "location": row.get("location") or "",
                  "link": row.get("link") or "",
                  "archived": row.get("archived") or "",
                  "verification": ver,
                  "description": row.get("description") or "",
                  "intensity": intensity
              }
              features.append({
                  "type": "Feature",
                  "geometry": {"type": "Point", "coordinates": [float(lon), float(lat)]},
                  "properties": props
              })

          with open("assets/data/events.geojson", "w", encoding="utf-8") as f:
              json.dump({"type": "FeatureCollection", "features": features}, f, ensure_ascii=False, indent=2)

          # --- EVENTS_TIMELINE.JSON ---
          tl_events = []
          for _, row in df.iterrows():
              try:
                  d_str = str(row.get("date", ""))
                  if '/' in d_str:
                      parts = d_str.split('/')
                      if len(parts) == 3: day, month, year = map(str.strip, parts)
                      if len(year) == 2: year = "20" + year
                  elif '-' in d_str:
                      parts = d_str.split('-')
                      if len(parts) == 3: year, month, day = map(str.strip, parts)
                  else: continue

                  html = f"Tipo: {row.get('type')}<br>Luogo: {row.get('location')}<br>Verifica: {row.get('verification')}"
                  
                  tl_events.append({
                      "start_date": {"year": int(year), "month": int(month), "day": int(day)},
                      "text": {"headline": row.get("title") or "Evento", "text": html},
                      "type": row.get("type"),
                      "date": f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                  })
              except: continue

          with open("assets/data/events_timeline.json", "w", encoding="utf-8") as f:
              json.dump({"events": tl_events}, f, ensure_ascii=False, indent=2)
          
          print("âœ… Dati sito rigenerati con successo.")
          EOF

      - name: Commit and push updates
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add assets/data/events.geojson assets/data/events_timeline.json
          git diff --quiet && git diff --staged --quiet || (git commit -m "Auto-update: AI + Data Refresh" && git push)
