name: AI Verification & Site Update

on:
  schedule:
    - cron: '0 */6 * * *' # Ogni 6 ore
  workflow_dispatch:      # Manuale

permissions:
  contents: write

jobs:
  ai-and-sync:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Scarica tutta la storia per evitare errori di sync

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install pandas requests numpy gspread google-auth tavily-python openai

      # --- STEP 1: AGENTE AI ---
      - name: ü§ñ Run AI Agent
        env:
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
        run: python3 scripts/ai_agent.py || echo "AI Agent finished with warnings (continuing...)"

      # --- STEP 2: GENERAZIONE DATI & CLASSIFICAZIONE ATTORI ---
      - name: üè≠ Process Data & Classify Actors
        run: |
          python3 <<'EOF'
          import pandas as pd
          import json
          import sys
          import numpy as np
          import requests
          import io
          import re

          # CONFIGURAZIONE
          SHEET_URL = "https://docs.google.com/spreadsheets/d/1NEyNXzCSprGOw6gCmVVbtwvFmz8160Oag-WqG93ouoQ/export?format=csv"
          
          print("üîÑ Scaricamento Database Completo...")
          try:
              response = requests.get(SHEET_URL)
              response.raise_for_status()
              df = pd.read_csv(io.StringIO(response.text))
          except Exception as e:
              print(f"‚ùå CRITICAL ERROR: {e}")
              sys.exit(1)

          # NORMALIZZAZIONE COLONNE
          df.columns = df.columns.str.strip().str.lower()
          
          def get_col(candidates):
              for c in candidates:
                  if c in df.columns: return c
              return None

          col_lat = get_col(['latitude', 'lat'])
          col_lon = get_col(['longitude', 'lon', 'long'])
          col_desc = get_col(['description', 'descrizione', 'notes', 'note'])
          col_loc = get_col(['location', 'luogo', 'city'])
          col_date = get_col(['date', 'data'])
          col_type = get_col(['type', 'tipo', 'type of attack'])
          col_title = get_col(['title', 'titolo'])
          col_link = get_col(['source', 'link', 'fonte'])
          col_video = get_col(['video', 'video_url'])
          col_ver = get_col(['verification', 'verifica'])
          col_int = get_col(['intensity', 'intensit√†'])
          
          if not col_lat or not col_lon:
              print("‚ùå Errore: Colonne coordinate non trovate.")
              sys.exit(1)

          # CLASSIFICAZIONE ATTORI
          def classify_actor(row):
              text = str(row.get(col_desc, '')).lower() + " " + str(row.get(col_title, '')).lower()
              loc = str(row.get(col_loc, '')).lower()
              
              if re.search(r'russian (force|army|missile|drone|shelling)|forces of rf|shahed|iskander|kalibr| FAB-\d+|kh-\d+', text):
                  return 'RUS'
              if re.search(r'ukrainian (force|army|missile|drone)|zsu|uaf|himars|atacms|storm shadow|magura', text):
                  return 'UKR'
              if re.search(r'belgorod|kursk|voronezh|rostov|crimea|sevastopol|kerch|moscow|krasnodar|bryansk', loc):
                  return 'UKR'
              if re.search(r'kyiv|kiev|kharkiv|kharkov|odesa|odessa|lviv|dnipro|zaporizhzhia|vinnytsia|sumy|poltava', loc):
                  return 'RUS'
              return 'UNK'

          # GENERAZIONE GEOJSON
          features = []
          stats = {'RUS': 0, 'UKR': 0, 'UNK': 0}

          for _, row in df.iterrows():
              try:
                  lat = float(row[col_lat])
                  lon = float(row[col_lon])
                  if pd.isna(lat) or pd.isna(lon): continue
              except: continue

              actor_code = classify_actor(row)
              stats[actor_code] += 1

              desc = str(row.get(col_desc, '')) if pd.notna(row.get(col_desc)) else ""
              loc = str(row.get(col_loc, '')) if pd.notna(row.get(col_loc)) else ""
              try: intensity = float(row.get(col_int) or 0.2)
              except: intensity = 0.2

              props = {
                  "title": str(row.get(col_title, 'Evento')),
                  "date": str(row.get(col_date, '')),
                  "type": str(row.get(col_type, 'General')),
                  "location": loc,
                  "link": str(row.get(col_link, '')),
                  "verification": str(row.get(col_ver, 'not verified')),
                  "description": desc,
                  "video": str(row.get(col_video, '')),
                  "intensity": intensity,
                  "actor_code": actor_code
              }
              features.append({
                  "type": "Feature",
                  "geometry": {"type": "Point", "coordinates": [lon, lat]},
                  "properties": props
              })

          with open("assets/data/events.geojson", "w", encoding="utf-8") as f:
              json.dump({"type": "FeatureCollection", "features": features}, f, ensure_ascii=False, indent=2)
          print(f"‚úÖ events.geojson: Generati {len(features)} eventi. Stats: {stats}")

          # GENERAZIONE TIMELINE
          tl_events = []
          for feat in features:
              p = feat['properties']
              try:
                  d_str = p['date']
                  year, month, day = "2024", "01", "01"
                  if '-' in d_str: year, month, day = d_str.split('-')[:3]
                  elif '/' in d_str: day, month, year = d_str.split('/')[:3]
                  if len(year) == 2: year = "20" + year

                  html = f"<b>Tipo:</b> {p['type']}<br><b>Attore:</b> {p['actor_code']}<br><b>Luogo:</b> {p['location']}<br><br>{p['description']}"
                  tl_obj = {
                      "start_date": {"year": int(year), "month": int(month), "day": int(day)},
                      "text": { "headline": p['title'], "text": html },
                      "group": p['type']
                  }
                  if p['video']: tl_obj['media'] = { "url": p['video'], "caption": "Fonte Video" }
                  tl_events.append(tl_obj)
              except: continue

          with open("assets/data/events_timeline.json", "w", encoding="utf-8") as f:
              json.dump({"title":{"text":{"headline":"Timeline"}}, "events":tl_events}, f, ensure_ascii=False)
          print(f"‚úÖ events_timeline.json: Generati {len(tl_events)} slide.")
          EOF

      # --- STEP 3: FORCE COMMIT (SOLUZIONE AL CONFLITTO) ---
      # Usiamo 'reset --soft' per allinearci al server PRIMA di committare.
      # Questo fa vincere sempre i file generati dall'Action.
      - name: Commit and push updates
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          
          # Scarica lo stato attuale del branch remoto
          git fetch origin main
          
          # Sposta l'HEAD locale sul remoto, MA mantiene i file generati (events.geojson) nell'area di lavoro
          git reset --soft origin/main
          
          # Ora aggiungiamo i file (sovrascriveranno quelli remoti nel prossimo commit)
          git add assets/data/events.geojson assets/data/events_timeline.json
          
          # Controlla se c'√® qualcosa da committare
          if git diff --cached --quiet; then
            echo "Nessuna modifica ai dati."
            exit 0
          fi
          
          # Crea un nuovo commit pulito sopra l'ultimo commit remoto
          git commit -m "Auto-update: Data Refresh & Actor Classification"
          
          # Push diretto (ora √® sicuro perch√© siamo allineati)
          git push origin main
